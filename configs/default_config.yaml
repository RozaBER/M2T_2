# Default configuration for Brain-to-Text Model

# Data configuration
data:
  data_root: "/data/zshao/MASC-MEG"
  subjects: null  # null for all subjects, or list like ["sub-01", "sub-02"]
  sessions: null  # null for all sessions
  tasks: null     # null for all tasks, or list like [0, 1, 2, 3]
  sampling_rate: 1000
  segment_length: 2.0  # seconds
  overlap: 0.25  # 25% overlap
  lowpass: 100.0
  highpass: 0.1
  normalize: true
  cache_dir: "./cache"
  num_workers: 4

# Model configuration
model:
  # MEG/EEG encoder
  n_channels: 208  # Number of MEG channels
  eeg_hidden_dim: 768
  num_encoder_layers: 12
  
  # RVQ configuration
  n_codebooks: 8
  codebook_size: 1024  # Reduced from 8192 to avoid k-means issues
  codebook_dim: 256
  
  # LLM configuration
  llm_model_name: "gpt2"
  freeze_llm_initially: true
  use_lora: true
  lora_r: 8
  lora_alpha: 16
  lora_dropout: 0.2
  
  # General
  dropout: 0.3
  max_seq_length: 2048
  max_text_length: 256
  
  # Optional: checkpoint to load
  checkpoint_path: null

# Tokenizer configuration
tokenizer:
  use_pretrained: true
  model_name: "gpt2"
  # Alternative: use custom tokenizer
  # use_pretrained: false
  # path: "./tokenizer"

# Training configuration - Optimized for 2x NVIDIA A5500 (24GB each)
training:
  # Training mode specific
  batch_size: 16  # 8 per GPU for better memory efficiency
  gradient_clip: 1.0
  accumulation_steps: 4  # Effective batch size of 64
  mixed_precision: false
  gradient_checkpointing: false  # Disabled due to compatibility issues
  
  # Learning schedule
  num_epochs: 30
  max_steps: null  # Set to override num_epochs
  warmup_steps: 2000  # Extended warmup for stability
  warmup_ratio: 0.1  # 10% of total steps
  learning_rate: 0.0001
  weight_decay: 0.01
  
  # Multi-phase training steps
  phase1_steps: 10000  # Encoder + RVQ reconstruction
  phase2_steps: 5000   # RVQ diversity training
  phase3_steps: 20000  # End-to-end fine-tuning
  phase4_steps: 10000  # Optional distillation
  
  # Distillation parameters
  distill_temperature: 3.0
  distill_alpha: 0.7
  
  # Checkpointing
  save_steps: 1000
  eval_steps: 500
  logging_steps: 10
  
  # Output
  output_dir: "./output"
  device: "cuda"
  log_wandb: true
  seed: 42
  
  # Multi-GPU settings
  distributed: true
  world_size: 2
  backend: "nccl"
  find_unused_parameters: false

# Evaluation configuration
evaluation:
  compute_generation_metrics: true
  metrics:
    - rouge  # ROUGE-1, ROUGE-2, ROUGE-L
    - bleu   # BLEU-1 through BLEU-4
  
  generation_params:
    max_length: 128
    num_beams: 4
    temperature: 0.7
    top_p: 0.9
    do_sample: false
    early_stopping: true
  
  evaluation_frequency: 5  # Evaluate every N epochs
  save_best_by_metric: "gen_rouge-l"  # Metric for model selection
  limit_eval_batches: null  # Use all validation data (only 8 samples)

# Inference configuration
inference:
  max_length: 256
  temperature: 0.8
  top_k: 50
  top_p: 0.95
  num_beams: 1
  do_sample: true
