# Core dependencies
torch>=2.0.0
transformers>=4.30.0
tokenizers>=0.13.0
numpy>=1.21.0
scipy>=1.7.0
pandas>=1.3.0
tqdm>=4.62.0
pyyaml>=5.4.0

# MEG/EEG processing
mne>=1.0.0
scikit-learn>=1.0.0

# Model components
einops>=0.6.0
vector-quantize-pytorch>=1.0.0

# Training utilities
wandb>=0.12.0
tensorboard>=2.8.0

# Evaluation
nltk>=3.7
rouge-score>=0.1.2
bert-score>=0.3.11
sacrebleu>=2.3.1

# Optional: for faster training
apex  # NVIDIA Apex for mixed precision training
flash-attn>=2.0.0  # Flash Attention for efficient transformer

# Development
pytest>=7.0.0
black>=22.0.0
flake8>=4.0.0
mypy>=0.950
jupyter>=1.0.0
matplotlib>=3.5.0
seaborn>=0.11.0
